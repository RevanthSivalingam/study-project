{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4fefeb72",
   "metadata": {},
   "source": [
    "### 1. IMPORT REQUIRED LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "68f32002",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import defaultdict\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7e9e0a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI(api_key=\"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\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94824ca2",
   "metadata": {},
   "source": [
    "###   2. DOCUMENT LOADER FUNCTION\n",
    "#####    Reads PDF, TXT, and JSON files - Converts all content into plain text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "fb91ce6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pdfplumber\n",
    "\n",
    "def load_documents(folder=\"inputfiles\"):\n",
    "    docs = []\n",
    "\n",
    "    for filename in os.listdir(folder):\n",
    "        filepath = os.path.join(folder, filename)\n",
    "\n",
    "        # ---- PDF FILES ----\n",
    "        if filename.lower().endswith(\".pdf\"):\n",
    "            with pdfplumber.open(filepath) as pdf:\n",
    "                text = \"\"\n",
    "                for page in pdf.pages:\n",
    "                    page_text = page.extract_text()\n",
    "                    if page_text:\n",
    "                        text += page_text + \"\\n\"\n",
    "                docs.append(text)\n",
    "\n",
    "        # ---- TEXT FILES ----\n",
    "        elif filename.lower().endswith(\".txt\"):\n",
    "            with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "                docs.append(f.read())\n",
    "\n",
    "        # ---- JSON FILES ----\n",
    "        elif filename.lower().endswith(\".json\"):\n",
    "            with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "                data = json.load(f)\n",
    "                # Convert JSON to readable string\n",
    "                docs.append(json.dumps(data, indent=2))\n",
    "\n",
    "    return docs\n",
    "\n",
    "\n",
    "documents = load_documents()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf59bf4",
   "metadata": {},
   "source": [
    "### 3.Section-Based Text Chunking Using Rule-Based Header Detection\n",
    "\n",
    "##### The function scans the document line by line, identifies section headings, and groups all text under that heading until the next section appears."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "773cfef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def section_chunk(text):\n",
    "    sections = []\n",
    "    current_title = None\n",
    "    buffer = []\n",
    "\n",
    "    for line in text.split(\"\\n\"):\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "\n",
    "        # Detect top-level section headers\n",
    "        is_header = (\n",
    "            re.match(r\"^PAGE\\s+\\d+\", line, re.IGNORECASE) or\n",
    "            re.match(r\"^\\d+\\.\\s+[A-Za-z]\", line)  \n",
    "        )\n",
    "\n",
    "        if is_header:\n",
    "            if current_title and buffer:\n",
    "                sections.append({\n",
    "                    \"title\": current_title,\n",
    "                    \"content\": \"\\n\".join(buffer)\n",
    "                })\n",
    "\n",
    "            current_title = line\n",
    "            buffer = [line]   \n",
    "        else:\n",
    "            buffer.append(line)\n",
    "\n",
    "    if current_title and buffer:\n",
    "        sections.append({\n",
    "            \"title\": current_title,\n",
    "            \"content\": \"\\n\".join(buffer)\n",
    "        })\n",
    "\n",
    "    return sections\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f402a1ce",
   "metadata": {},
   "source": [
    "### 4.Semantic Embedding of Document Sections\n",
    "\n",
    "##### Each extracted section is transformed into a dense vector representation using the all-mpnet-base-v2 embedding model. These embeddings enable semantic similarity comparison and efficient information retrieval in downstream tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "89e45ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder = SentenceTransformer(\"all-mpnet-base-v2\")\n",
    "\n",
    "sections = []\n",
    "for doc in documents:\n",
    "    sections.extend(section_chunk(doc))\n",
    "\n",
    "section_texts = [s[\"title\"] + \" \" + s[\"content\"] for s in sections]\n",
    "section_embeddings = embedder.encode(section_texts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40554564",
   "metadata": {},
   "source": [
    "### Generic Term Identification using TF-IDF (Corpus-Level)\n",
    "\n",
    "##### This function identifies common terms across document sections using TF-IDF vectorization. Terms that appear in a high percentage of sections are considered generic and filtered out during retrieval to enhance relevance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "47ee8186",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_generic_terms(sections, top_percent=0.15):\n",
    "    texts = [s[\"title\"] + \" \" + s[\"content\"] for s in sections]\n",
    "\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        stop_words=\"english\",\n",
    "        max_df=0.95,     \n",
    "        min_df=2\n",
    "    )\n",
    "\n",
    "    tfidf = vectorizer.fit_transform(texts)\n",
    "    terms = np.array(vectorizer.get_feature_names_out())\n",
    "\n",
    "    # Mean TF-IDF per term across corpus\n",
    "    mean_scores = tfidf.mean(axis=0).A1\n",
    "\n",
    "    # Lowest TF-IDF â†’ most generic\n",
    "    cutoff = int(len(terms) * top_percent)\n",
    "    generic_terms = set(terms[np.argsort(mean_scores)[:cutoff]])\n",
    "\n",
    "    return generic_terms\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee450388",
   "metadata": {},
   "source": [
    "### Learning using TF-IDF\n",
    "\n",
    "##### This function learns the most informative unigrams and bigrams from section content using TF-IDF scoring.The highest-scoring terms represent domain-specific keywords that help guide query understanding and rule-based retriev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a26a0653",
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn_key_terms(sections, top_k=40):\n",
    "    texts = [s[\"content\"] for s in sections if s[\"content\"]]\n",
    "\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        stop_words=\"english\",\n",
    "        ngram_range=(1, 2),\n",
    "        max_df=0.85\n",
    "    )\n",
    "    X = vectorizer.fit_transform(texts)\n",
    "\n",
    "    terms = vectorizer.get_feature_names_out()\n",
    "    scores = X.mean(axis=0).A1\n",
    "\n",
    "    ranked = sorted(zip(terms, scores), key=lambda x: x[1], reverse=True)\n",
    "    return [t for t, _ in ranked[:top_k]]\n",
    "\n",
    "learned_terms = learn_key_terms(sections)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5d9e5b",
   "metadata": {},
   "source": [
    "### Automatic Rule-Based Knowledge Graph Construction\n",
    "\n",
    "##### This function automatically builds a lightweight, in-memory knowledge structure by linking learned key terms to relevant section lines.It captures descriptive statements and numeric constraints using simple pattern matching, enabling structured, explainable knowledge lookup without a graph database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "14a71a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "knowledge_graph = defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "def build_kg_automatically(sections, learned_terms):\n",
    "    for sec in sections:\n",
    "        content = sec[\"content\"]\n",
    "        if not content:\n",
    "            continue\n",
    "\n",
    "        for line in content.split(\"\\n\"):\n",
    "            line_l = line.lower()\n",
    "\n",
    "            matched_terms = [t for t in learned_terms if t in line_l]\n",
    "            if not matched_terms:\n",
    "                continue\n",
    "\n",
    "            numbers = re.findall(r\"\\d+\\s+(days|weeks|months)\", line_l)\n",
    "\n",
    "            for term in matched_terms:\n",
    "                if numbers:\n",
    "                    knowledge_graph[term][\"limits\"].append(line.strip())\n",
    "                else:\n",
    "                    knowledge_graph[term][\"description\"].append(line.strip())\n",
    "\n",
    "build_kg_automatically(sections, learned_terms)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c501a34",
   "metadata": {},
   "source": [
    "### Rule-Based Knowledge Graph Querying\n",
    "\n",
    "##### This function performs a lightweight knowledge lookup by matching query terms against the in-memory knowledge structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8cf3b0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_knowledge_graph(query):\n",
    "    q = query.lower()\n",
    "    collected = []\n",
    "\n",
    "    for entity, facts in knowledge_graph.items():\n",
    "        if entity in q:\n",
    "            for v in facts.values():\n",
    "                collected.extend(v)\n",
    "\n",
    "    return collected if collected else None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdec026b",
   "metadata": {},
   "source": [
    "### Corpus Construction for Semantic Retrieval\n",
    "##### This function builds a corpus of section texts for semantic retrieval. It combines section titles and content into single text entries, which are then used for embedding and similarity search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "aa76b457",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_corpus(sections):\n",
    "    return [(s[\"title\"] + \" \" + s[\"content\"]).lower() for s in sections]\n",
    "\n",
    "corpus = build_corpus(sections)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8b0f52",
   "metadata": {},
   "source": [
    "### Clustering Document Sections Using K-Means\n",
    "\n",
    "##### This block groups semantically similar document sections into clusters using K-Means on their embeddings, enabling topic-based organization. Cluster centroids are then computed to represent the average semantic meaning of each cluster for efficient retrieval and matching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "65f419b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLUSTERS = 6\n",
    "kmeans = KMeans(n_clusters=NUM_CLUSTERS, random_state=42)\n",
    "cluster_ids = kmeans.fit_predict(section_embeddings)\n",
    "\n",
    "cluster_map = {}\n",
    "for i, cid in enumerate(cluster_ids):\n",
    "    cluster_map.setdefault(cid, []).append(sections[i])\n",
    "\n",
    "cluster_centroids = {\n",
    "    cid: np.mean(\n",
    "        embedder.encode([s[\"title\"] + \" \" + s[\"content\"] for s in sec]),\n",
    "        axis=0\n",
    "    )\n",
    "    for cid, sec in cluster_map.items()\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "726a9f70",
   "metadata": {},
   "source": [
    "### Relevant Cluster Selection Using Semantic Similarity\n",
    "\n",
    "##### This function converts the user query into an embedding and compares it with each cluster centroid using cosine similarity. The cluster with the highest similarity score is selected as the most relevant group of document sections for answering the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "62db19b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_cluster(query):\n",
    "    q_emb = embedder.encode([query])[0]\n",
    "    scores = {\n",
    "        cid: cosine_similarity([q_emb], [centroid])[0][0]\n",
    "        for cid, centroid in cluster_centroids.items()\n",
    "    }\n",
    "    return max(scores, key=scores.get)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f577cecb",
   "metadata": {},
   "source": [
    "### Learning Corpus-Wide Generic Terms\n",
    "\n",
    "##### This step identifies frequently occurring, low-information terms across all document sections using TF-IDF statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7b0ce91c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generic terms learned: ['task', 'arrangement', 'download', 'attendance', 'insufficient', 'measurable', 'installation', 'learning', 'effectiveness', 'accrual', 'outside', 'chair', 'pro', 'submitted', '80', 'december', 'strategic', 'parents', 'dependent', 'july', 'does', 'activities', 'address', 'examples', 'updated', 'deadlines', 'possible', 'budget', 'dates', 'permanent', 'prepared', 'longer', 'timeline', 'standard', 'evaluated', 'bonuses', 'rated', 'tickets', 'guide', 'individual', 'ins', 'higher', 'pickup', 'messages', 'smart', 'maintaining', 'limit', 'peers', 'stipend', 'hour', 'emails', 'receipts', 'worked', 'innovation', 'overall', 'previous', 'preparation', 'arm', 'amounts', 'responsiveness', 'open', 'locations', 'answer', 'signed', 'matters', '401', 'public', 'online', 'payment', 'thinking', 'marketing', 'following', 'adjustment', 'march', 'camera', 'rate', 'sales', 'career', 'colleagues', 'acknowledgment', '5500', 'tied', 'clearly', 'participation', 'update', 'general', 'february', 'scheduled', 'attend', 'active']\n"
     ]
    }
   ],
   "source": [
    "GENERIC_TERMS = build_generic_terms(sections)\n",
    "print(f\"Generic terms learned: {list(GENERIC_TERMS)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c93625",
   "metadata": {},
   "source": [
    "### Query Normalization for Knowledge-Guided Retrieval\n",
    "\n",
    "##### This function removes corpus-wide generic terms from the user query to emphasize informative keywords.The normalized query improves both rule-based knowledge lookup and semantic section retrieval accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "6006158b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_query_for_retrieval(query):\n",
    "    tokens = query.lower().split()\n",
    "    filtered = [t for t in tokens if t not in GENERIC_TERMS]\n",
    "    return \" \".join(filtered) if filtered else query\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65a172a",
   "metadata": {},
   "source": [
    "### Best Section Selection Using Similarity Threshold\n",
    "\n",
    "##### This function identifies the most relevant section by comparing the query embedding with section title embeddings using cosine similarity. A similarity threshold ensures that only sufficiently relevant sections are selected, improving answer precision and reducing noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ac22cfac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_best_section(sections, query, threshold=0.45):\n",
    "    q_emb = embedder.encode([query])[0]\n",
    "\n",
    "    # ---- TITLE MATCHING ----\n",
    "    titles = [s[\"title\"] for s in sections]\n",
    "    title_embeddings = embedder.encode(titles)\n",
    "    title_scores = cosine_similarity([q_emb], title_embeddings)[0]\n",
    "\n",
    "    # ---- CONTENT MATCHING ----\n",
    "    contents = [s[\"content\"] for s in sections]\n",
    "    content_embeddings = embedder.encode(contents)\n",
    "    content_scores = cosine_similarity([q_emb], content_embeddings)[0]\n",
    "\n",
    "    # ---- COMBINE SCORES (Weighted) ----\n",
    "    combined_scores = [\n",
    "        0.6 * title_scores[i] + 0.4 * content_scores[i]\n",
    "        for i in range(len(sections))\n",
    "    ]\n",
    "\n",
    "    best_idx = int(np.argmax(combined_scores))\n",
    "    #best_score = combined_scores[best_idx]\n",
    "\n",
    "    return sections[best_idx]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ff2009",
   "metadata": {},
   "source": [
    "### Section Relevance Scoring with Length Normalization\n",
    "\n",
    "##### This function computes semantic similarity between a query and a document section using embeddings.A length-normalized score is applied to favor content-rich sections while filtering out weak matches using a relevance threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a3c44865",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_section_relevant(section, query, threshold=0.55):\n",
    "    section_text = (section[\"title\"] + \" \" + section[\"content\"]).strip()\n",
    "\n",
    "    sec_emb = embedder.encode(section_text)\n",
    "    qry_emb = embedder.encode(query)\n",
    "\n",
    "    score = cosine_similarity([sec_emb], [qry_emb])[0][0]\n",
    "\n",
    "    # ðŸ”§ LENGTH NORMALIZATION (NO keywords)\n",
    "    length_factor = min(len(section_text) / 300, 1.0)\n",
    "    score = score * (0.7 + 0.3 * length_factor)\n",
    "\n",
    "    return score >= threshold\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b14e06",
   "metadata": {},
   "source": [
    "### Knowledge-Guided Section Grouping and Selection\n",
    "\n",
    "##### These functions group knowledge graphâ€“retrieved facts by their originating document sections and select the most relevant section using semantic similarity with lexical overlap.This enables precise, explainable section-level retrieval guided by lightweight structured knowledge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c1fbd950",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_by_section(sections, candidate_lines):\n",
    "    section_map = {}\n",
    "    for sec in sections:\n",
    "        sec_lines = sec[\"content\"].split(\"\\n\")\n",
    "        matched = [l for l in sec_lines if l in candidate_lines]\n",
    "        if matched:\n",
    "            section_map[sec[\"title\"]] = matched\n",
    "    return section_map\n",
    "\n",
    "\n",
    "def select_best_kg_section(query, section_map):\n",
    "    if not section_map:\n",
    "        return None, None\n",
    "\n",
    "    query_l = query.lower()\n",
    "    query_terms = set(query_l.split())\n",
    "\n",
    "    titles = list(section_map.keys())\n",
    "    title_embs = embedder.encode(titles)\n",
    "    q_emb = embedder.encode([query])[0]\n",
    "\n",
    "    sims = cosine_similarity([q_emb], title_embs)[0]\n",
    "\n",
    "    final_scores = []\n",
    "\n",
    "    for i, title in enumerate(titles):\n",
    "        title_l = title.lower()\n",
    "\n",
    "        # lexical overlap bonus (NO hardcoding)\n",
    "        overlap = sum(1 for t in query_terms if t in title_l)\n",
    "\n",
    "        # final score\n",
    "        score = sims[i] + (0.25 * overlap)\n",
    "        final_scores.append(score)\n",
    "\n",
    "    best_idx = max(range(len(final_scores)), key=lambda i: final_scores[i])\n",
    "\n",
    "    return titles[best_idx], section_map[titles[best_idx]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f712df0",
   "metadata": {},
   "source": [
    "### Maximal Marginal Relevance (MMR) for Diverse Sentence Selection\n",
    "\n",
    "##### This function selects the most relevant yet diverse sentences by balancing query relevance and redundancy using the MMR algorithm. It ensures that retrieved results are both highly related to the query and minimally repetitive, improving answer quality and coverage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "0d101481",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mmr(query, sentences, k=6, lambda_param=0.7):\n",
    "    #Gives 70% importance to relevance,Gives 30% importance to diversity\n",
    "    sent_embs = embedder.encode(sentences)\n",
    "    q_emb = embedder.encode([query])[0]\n",
    "\n",
    "    selected = []\n",
    "    used = set()\n",
    "\n",
    "    for _ in range(min(k, len(sentences))):\n",
    "        scores = []\n",
    "        for i, emb in enumerate(sent_embs):\n",
    "            if i in used:\n",
    "                continue\n",
    "            relevance = cosine_similarity([q_emb], [emb])[0][0]\n",
    "            diversity = max(\n",
    "                [cosine_similarity([emb], [sent_embs[j]])[0][0] for j in used],\n",
    "                default=0\n",
    "            )\n",
    "            score = lambda_param * relevance - (1 - lambda_param) * diversity\n",
    "            scores.append((score, i))\n",
    "\n",
    "        if not scores:\n",
    "            break\n",
    "\n",
    "        best = max(scores)[1]\n",
    "        used.add(best)\n",
    "        selected.append(sentences[best])\n",
    "\n",
    "    return selected\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ac39bf",
   "metadata": {},
   "source": [
    "### Prompt Construction for Grounded LLM Responses\n",
    "\n",
    "##### This function constructs a strict, context-bound prompt that limits the LLM to retrieved policy content only.It explicitly prevents hallucination by instructing the model to avoid external knowledge and unsupported generalizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "175c6a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_llm_prompt(query, retrieved_sentences):\n",
    "\n",
    "    context = \"\\n\".join(f\"- {s}\" for s in retrieved_sentences)\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "You are an enterprise HR policy assistant.\n",
    "\n",
    "Answer the question using ONLY the information provided in the context below.\n",
    "Do NOT add external knowledge.\n",
    "Do NOT generalize.\n",
    "If a detail is not explicitly mentioned, say \"Not specified in the policy\".\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{query}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "    return prompt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caae4d07",
   "metadata": {},
   "source": [
    "### LLM-Based Answer Generation\n",
    "\n",
    "##### This function invokes a language model to generate a response based strictly on the constructed prompt and retrieved context.A low temperature is used to ensure deterministic, policy-grounded answers suitable for enterprise use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "98e73f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_llm_answer(prompt):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",   # or gpt-3.5-turbo\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful enterprise policy assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=0.2\n",
    "    )\n",
    "    return response.choices[0].message.content.strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5299179",
   "metadata": {},
   "source": [
    "### Content Richness Validation for Sections\n",
    "\n",
    "##### This function checks whether a document section contains sufficient meaningful text to be used for retrieval and answer generation.It prevents title-only or sparse sections from being selected, improving retrieval quality and response relevance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "7143b91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_content_rich(section):\n",
    "    return (\n",
    "        len(section[\"content\"].strip()) > 60 and\n",
    "        len(section[\"content\"].split()) > 10\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13507225",
   "metadata": {},
   "source": [
    "### End-to-End Query Answering with Knowledge-Guided RAG\n",
    "\n",
    "##### This function answers user queries by first leveraging rule-based structured knowledge to identify relevant sections, followed by semantic fallback when needed.The selected content is refined using MMR and passed to an LLM to generate a grounded, context-aware response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "38936035",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_query(query):\n",
    "\n",
    "    # STEP 1: Knowledge Graph candidate generation\n",
    "    normalized_query = normalize_query_for_retrieval(query)\n",
    "    kg_candidates = query_knowledge_graph(normalized_query)\n",
    "\n",
    "    if kg_candidates:\n",
    "        grouped = group_by_section(sections, kg_candidates)\n",
    "        best_title, _ = select_best_kg_section(normalized_query, grouped)\n",
    "\n",
    "        if best_title:\n",
    "            candidate = next(s for s in sections if s[\"title\"] == best_title)\n",
    "\n",
    "\n",
    "            if not is_content_rich(candidate):\n",
    "               print(\"Title-only section detected, falling back to content section\")\n",
    "\n",
    "                # fallback: closest section with real content\n",
    "               content_sections = [s for s in sections if is_content_rich(s)]\n",
    "               # reuse your existing semantic logic\n",
    "               candidate = select_best_section(content_sections, normalized_query)\n",
    "\n",
    "            full_section = candidate\n",
    "            #print(\"SELECTED SECTION:\", full_section[\"title\"])\n",
    "            #print(\"CONTENT PREVIEW:\", full_section[\"content\"][:300])\n",
    "\n",
    "            #normalized_query = normalize_query_for_retrieval(query)\n",
    "            sentences = sent_tokenize(full_section[\"content\"])\n",
    "            final = mmr(normalized_query, sentences)\n",
    "            retrieved_indices = [sentences.index(s) for s in final if s in sentences]\n",
    "\n",
    "            #print(\"MMR SENTENCES:\", final)\n",
    "\n",
    "            contextual_query = f\"{full_section['title']} details\"\n",
    "            prompt = build_llm_prompt(contextual_query, final)\n",
    "\n",
    "            try:\n",
    "              llm_answer = \" \".join(final)\n",
    "            except Exception as e:\n",
    "              llm_answer = \"LLM unavailable due to quota limit. Showing retrieved context only.\"\n",
    "              print(\"LLM Error:\", e)            \n",
    "\n",
    "            print(\"\\n===== ANSWER (KG + RAG + LLM) =====\\n\")\n",
    "            print(f\" Section: {best_title}\\n\")\n",
    "            print(\"LLM answer\")\n",
    "            print(llm_answer)\n",
    "            return {\n",
    "            \"answer\": llm_answer,\n",
    "            \"retrieved_indices\": retrieved_indices,\n",
    "            \"sentences\": sentences\n",
    "            }\n",
    "\n",
    "    # STEP 2: Semantic fallback\n",
    "    cluster_id = select_cluster(normalized_query)\n",
    "    cluster_sections = cluster_map[cluster_id]\n",
    "\n",
    "    full_section = select_best_section(cluster_sections, normalized_query)\n",
    "\n",
    "    if not full_section:\n",
    "        print(\"\\n===== ANSWER =====\\n\")\n",
    "        print(\"The requested information is not available in the current knowledge base.\")\n",
    "        return\n",
    "\n",
    "    #normalized_query = normalize_query_for_retrieval(query)\n",
    "    sentences = sent_tokenize(full_section[\"content\"])\n",
    "    final = mmr(normalized_query, sentences)\n",
    "    retrieved_indices = [sentences.index(s) for s in final if s in sentences]\n",
    "\n",
    "    print(\"MMR SENTENCES:\", final)\n",
    "\n",
    "    contextual_query = f\"{full_section['title']} details\"\n",
    "    prompt = build_llm_prompt(contextual_query, final)\n",
    "\n",
    "    try:\n",
    "       llm_answer =\" \".join(final)\n",
    "    except Exception as e:\n",
    "       llm_answer = \"LLM unavailable due to quota limit. Showing retrieved context only.\"\n",
    "       print(\"LLM Error:\", e)\n",
    "\n",
    "    print(\"\\n===== ANSWER (RAG + LLM) =====\\n\")\n",
    "    print(f\"Section: {full_section['title']}\\n\")\n",
    "    print(\"LLM answer\")\n",
    "    print(llm_answer)\n",
    "    return {\n",
    "    \"answer\": llm_answer,\n",
    "    \"retrieved_indices\": retrieved_indices,\n",
    "    \"sentences\": sentences\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "343113c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_at_k(retrieved, relevant, k):\n",
    "    \"\"\"\n",
    "    Precision@K = (# relevant retrieved in top K) / K\n",
    "    retrieved: ranked list of sentence indices\n",
    "    relevant: list of relevant sentence indices\n",
    "    k: cutoff\n",
    "    \"\"\"\n",
    "    if k == 0:\n",
    "        return 0.0\n",
    "\n",
    "    retrieved_k = retrieved[:k]\n",
    "    rel_count = len(set(retrieved_k) & set(relevant))\n",
    "\n",
    "    return rel_count / k\n",
    "\n",
    "def recall_at_k(retrieved, relevant, k):\n",
    "    \"\"\"\n",
    "    Recall@K = (# relevant retrieved in top K) / (# relevant)\n",
    "    retrieved: ranked list of sentence indices\n",
    "    relevant: list of relevant sentence indices\n",
    "    k: cutoff\n",
    "    \"\"\"\n",
    "    if not relevant:\n",
    "        return 0.0\n",
    "\n",
    "    retrieved_k = retrieved[:k]\n",
    "    rel_count = len(set(retrieved_k) & set(relevant))\n",
    "\n",
    "    return rel_count / len(relevant)\n",
    "\n",
    "\n",
    "def mean_reciprocal_rank_single(retrieved, relevant):\n",
    "    \"\"\"\n",
    "    Computes MRR for a single query\n",
    "    retrieved: ranked list of sentence indices\n",
    "    relevant: list of relevant sentence indices\n",
    "    \"\"\"\n",
    "    for rank, idx in enumerate(retrieved, start=1):\n",
    "        if idx in relevant:\n",
    "            return 1 / rank\n",
    "    return 0.0\n",
    "def confidence_score_from_retrieval(retrieved, k=5):\n",
    "    if not retrieved:\n",
    "        return 0.0\n",
    "    return round(min(len(retrieved), k) / k, 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "7154712d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title-only section detected, falling back to content section\n",
      "\n",
      "===== ANSWER (KG + RAG + LLM) =====\n",
      "\n",
      " Section: PAGE 4 - MATERNITY LEAVE POLICY\n",
      "\n",
      "LLM answer\n",
      "MATERNITY LEAVE POLICY\n",
      "4.1 Eligibility:\n",
      "Female employees who have completed 6 months of continuous service are eligible\n",
      "for maternity leave. 4.2 Leave Duration:\n",
      "- Total maternity leave: 16 weeks (112 days)\n",
      "- Can be taken up to 4 weeks before expected delivery date\n",
      "- Minimum 12 weeks must be taken after delivery\n",
      "4.3 Paid Leave:\n",
      "- First 12 weeks: 100% of base salary\n",
      "- Weeks 13-16: 50% of base salary\n",
      "- Benefits continue during entire leave period\n",
      "4.4 Application Process: 4.\n"
     ]
    }
   ],
   "source": [
    "result = answer_query(\"pregnancy leave policy\")\n",
    "retrieved = result[\"retrieved_indices\"]\n",
    "sentences = result[\"sentences\"]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "66388381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision@3: 0.333\n",
      "Recall@3: 1.000\n",
      "MRR: 1.000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "\n",
    "# ---------------------------------\n",
    "# Build corpus ONLY from the section\n",
    "# ---------------------------------\n",
    "corpus = sentences\n",
    "query_text = \"pregnancy leave policy\"\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words=\"english\")\n",
    "tfidf_matrix = vectorizer.fit_transform(corpus)\n",
    "query_vec = vectorizer.transform([query_text])\n",
    "\n",
    "# ---------------------------------\n",
    "# Compute relevance scores\n",
    "# ---------------------------------\n",
    "similarity_scores = (tfidf_matrix @ query_vec.T).toarray().ravel()\n",
    "\n",
    "# ---------------------------------\n",
    "# Define relevant sentences\n",
    "# Rule: sentences with score >= mean score\n",
    "# ---------------------------------\n",
    "mean_score = np.mean(similarity_scores)\n",
    "relevant = [\n",
    "    i for i, score in enumerate(similarity_scores)\n",
    "    if score >= mean_score\n",
    "]\n",
    "\n",
    "# ---------------------------------\n",
    "# Metrics\n",
    "# ---------------------------------\n",
    "k = min(3, len(retrieved))\n",
    "\n",
    "precision = precision_at_k(retrieved, relevant, k)\n",
    "recall = recall_at_k(retrieved, relevant, k)\n",
    "mrr = mean_reciprocal_rank_single(retrieved, relevant)\n",
    "\n",
    "print(f\"Precision@{k}: {precision:.3f}\")\n",
    "print(f\"Recall@{k}: {recall:.3f}\")\n",
    "print(f\"MRR: {mrr:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f896ee53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gradio\n",
      "  Downloading gradio-6.5.1-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting aiofiles<25.0,>=22.0 (from gradio)\n",
      "  Downloading aiofiles-24.1.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in c:\\users\\91979\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gradio) (4.11.0)\n",
      "Collecting brotli>=1.1.0 (from gradio)\n",
      "  Downloading brotli-1.2.0-cp310-cp310-win_amd64.whl.metadata (6.3 kB)\n",
      "Collecting fastapi<1.0,>=0.115.2 (from gradio)\n",
      "  Downloading fastapi-0.128.0-py3-none-any.whl.metadata (30 kB)\n",
      "Collecting ffmpy (from gradio)\n",
      "  Downloading ffmpy-1.0.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting gradio-client==2.0.3 (from gradio)\n",
      "  Downloading gradio_client-2.0.3-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting groovy~=0.1 (from gradio)\n",
      "  Downloading groovy-0.1.2-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: httpx<1.0,>=0.24.1 in c:\\users\\91979\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gradio) (0.28.1)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=0.33.5 in c:\\users\\91979\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gradio) (0.36.0)\n",
      "Requirement already satisfied: jinja2<4.0 in c:\\users\\91979\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gradio) (3.1.6)\n",
      "Requirement already satisfied: markupsafe<4.0,>=2.0 in c:\\users\\91979\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gradio) (3.0.3)\n",
      "Requirement already satisfied: numpy<3.0,>=1.0 in c:\\users\\91979\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gradio) (2.2.6)\n",
      "Collecting orjson~=3.0 (from gradio)\n",
      "  Downloading orjson-3.11.6-cp310-cp310-win_amd64.whl.metadata (43 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\91979\\appdata\\roaming\\python\\python310\\site-packages (from gradio) (25.0)\n",
      "Requirement already satisfied: pandas<4.0,>=1.0 in c:\\users\\91979\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gradio) (2.3.3)\n",
      "Requirement already satisfied: pillow<13.0,>=8.0 in c:\\users\\91979\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gradio) (12.0.0)\n",
      "Requirement already satisfied: pydantic<=3.0,>=2.0 in c:\\users\\91979\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gradio) (2.12.4)\n",
      "Collecting pydub (from gradio)\n",
      "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting python-multipart>=0.0.18 (from gradio)\n",
      "  Downloading python_multipart-0.0.22-py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: pytz>=2017.2 in c:\\users\\91979\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gradio) (2025.2)\n",
      "Requirement already satisfied: pyyaml<7.0,>=5.0 in c:\\users\\91979\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gradio) (6.0.3)\n",
      "Collecting safehttpx<0.2.0,>=0.1.7 (from gradio)\n",
      "  Downloading safehttpx-0.1.7-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting semantic-version~=2.0 (from gradio)\n",
      "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
      "Collecting starlette<1.0,>=0.40.0 (from gradio)\n",
      "  Downloading starlette-0.52.1-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting tomlkit<0.14.0,>=0.12.0 (from gradio)\n",
      "  Downloading tomlkit-0.13.3-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting typer<1.0,>=0.12 (from gradio)\n",
      "  Downloading typer-0.21.1-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in c:\\users\\91979\\appdata\\roaming\\python\\python310\\site-packages (from gradio) (4.15.0)\n",
      "Collecting uvicorn>=0.14.0 (from gradio)\n",
      "  Downloading uvicorn-0.40.0-py3-none-any.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: fsspec in c:\\users\\91979\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gradio-client==2.0.3->gradio) (2025.10.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\91979\\appdata\\roaming\\python\\python310\\site-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\91979\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from anyio<5.0,>=3.0->gradio) (3.11)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\91979\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
      "Collecting starlette<1.0,>=0.40.0 (from gradio)\n",
      "  Downloading starlette-0.50.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting annotated-doc>=0.0.2 (from fastapi<1.0,>=0.115.2->gradio)\n",
      "  Downloading annotated_doc-0.0.4-py3-none-any.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: certifi in c:\\users\\91979\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from httpx<1.0,>=0.24.1->gradio) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\91979\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from httpx<1.0,>=0.24.1->gradio) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\91979\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from httpcore==1.*->httpx<1.0,>=0.24.1->gradio) (0.16.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\91979\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (3.20.0)\n",
      "Requirement already satisfied: requests in c:\\users\\91979\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (2.32.5)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\91979\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (4.67.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\91979\\appdata\\roaming\\python\\python310\\site-packages (from pandas<4.0,>=1.0->gradio) (2.9.0.post0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\91979\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas<4.0,>=1.0->gradio) (2025.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\91979\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pydantic<=3.0,>=2.0->gradio) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in c:\\users\\91979\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pydantic<=3.0,>=2.0->gradio) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in c:\\users\\91979\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pydantic<=3.0,>=2.0->gradio) (0.4.2)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\91979\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from typer<1.0,>=0.12->gradio) (8.3.1)\n",
      "Collecting shellingham>=1.3.0 (from typer<1.0,>=0.12->gradio)\n",
      "  Downloading shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rich>=10.11.0 (from typer<1.0,>=0.12->gradio)\n",
      "  Downloading rich-14.3.1-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\91979\\appdata\\roaming\\python\\python310\\site-packages (from click>=8.0.0->typer<1.0,>=0.12->gradio) (0.4.6)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\91979\\appdata\\roaming\\python\\python310\\site-packages (from python-dateutil>=2.8.2->pandas<4.0,>=1.0->gradio) (1.17.0)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich>=10.11.0->typer<1.0,>=0.12->gradio)\n",
      "  Downloading markdown_it_py-4.0.0-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\91979\\appdata\\roaming\\python\\python310\\site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.2)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio)\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\91979\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->huggingface-hub<2.0,>=0.33.5->gradio) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\91979\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->huggingface-hub<2.0,>=0.33.5->gradio) (2.5.0)\n",
      "Downloading gradio-6.5.1-py3-none-any.whl (24.2 MB)\n",
      "   ---------------------------------------- 0.0/24.2 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 3.9/24.2 MB 19.6 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 6.6/24.2 MB 16.1 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 9.2/24.2 MB 15.0 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 12.1/24.2 MB 14.5 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 14.7/24.2 MB 14.2 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 17.6/24.2 MB 14.0 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 20.4/24.2 MB 13.9 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 22.8/24.2 MB 13.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 24.2/24.2 MB 13.3 MB/s  0:00:01\n",
      "Downloading gradio_client-2.0.3-py3-none-any.whl (55 kB)\n",
      "Downloading aiofiles-24.1.0-py3-none-any.whl (15 kB)\n",
      "Downloading fastapi-0.128.0-py3-none-any.whl (103 kB)\n",
      "Downloading groovy-0.1.2-py3-none-any.whl (14 kB)\n",
      "Downloading orjson-3.11.6-cp310-cp310-win_amd64.whl (136 kB)\n",
      "Downloading safehttpx-0.1.7-py3-none-any.whl (9.0 kB)\n",
      "Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
      "Downloading starlette-0.50.0-py3-none-any.whl (74 kB)\n",
      "Downloading tomlkit-0.13.3-py3-none-any.whl (38 kB)\n",
      "Downloading typer-0.21.1-py3-none-any.whl (47 kB)\n",
      "Downloading annotated_doc-0.0.4-py3-none-any.whl (5.3 kB)\n",
      "Downloading brotli-1.2.0-cp310-cp310-win_amd64.whl (369 kB)\n",
      "Downloading python_multipart-0.0.22-py3-none-any.whl (24 kB)\n",
      "Downloading rich-14.3.1-py3-none-any.whl (309 kB)\n",
      "Downloading markdown_it_py-4.0.0-py3-none-any.whl (87 kB)\n",
      "Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Downloading uvicorn-0.40.0-py3-none-any.whl (68 kB)\n",
      "Downloading ffmpy-1.0.0-py3-none-any.whl (5.6 kB)\n",
      "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
      "Installing collected packages: pydub, brotli, tomlkit, shellingham, semantic-version, python-multipart, orjson, mdurl, groovy, ffmpy, annotated-doc, aiofiles, uvicorn, markdown-it-py, starlette, rich, typer, safehttpx, gradio-client, fastapi, gradio\n",
      "\n",
      "   ----------------------------------------  0/21 [pydub]\n",
      "   --- ------------------------------------  2/21 [tomlkit]\n",
      "   --- ------------------------------------  2/21 [tomlkit]\n",
      "   ----- ----------------------------------  3/21 [shellingham]\n",
      "   --------- ------------------------------  5/21 [python-multipart]\n",
      "   ------------- --------------------------  7/21 [mdurl]\n",
      "   ------------------- -------------------- 10/21 [annotated-doc]\n",
      "   -------------------- ------------------- 11/21 [aiofiles]\n",
      "   ---------------------- ----------------- 12/21 [uvicorn]\n",
      "   ---------------------- ----------------- 12/21 [uvicorn]\n",
      "   ---------------------- ----------------- 12/21 [uvicorn]\n",
      "   ---------------------- ----------------- 12/21 [uvicorn]\n",
      "   ------------------------ --------------- 13/21 [markdown-it-py]\n",
      "   ------------------------ --------------- 13/21 [markdown-it-py]\n",
      "   ------------------------ --------------- 13/21 [markdown-it-py]\n",
      "   ------------------------ --------------- 13/21 [markdown-it-py]\n",
      "   ------------------------ --------------- 13/21 [markdown-it-py]\n",
      "   ------------------------ --------------- 13/21 [markdown-it-py]\n",
      "   ------------------------ --------------- 13/21 [markdown-it-py]\n",
      "   -------------------------- ------------- 14/21 [starlette]\n",
      "   -------------------------- ------------- 14/21 [starlette]\n",
      "   -------------------------- ------------- 14/21 [starlette]\n",
      "   -------------------------- ------------- 14/21 [starlette]\n",
      "   ---------------------------- ----------- 15/21 [rich]\n",
      "   ---------------------------- ----------- 15/21 [rich]\n",
      "   ---------------------------- ----------- 15/21 [rich]\n",
      "   ---------------------------- ----------- 15/21 [rich]\n",
      "   ---------------------------- ----------- 15/21 [rich]\n",
      "   ---------------------------- ----------- 15/21 [rich]\n",
      "   ---------------------------- ----------- 15/21 [rich]\n",
      "   ---------------------------- ----------- 15/21 [rich]\n",
      "   ---------------------------- ----------- 15/21 [rich]\n",
      "   ---------------------------- ----------- 15/21 [rich]\n",
      "   ---------------------------- ----------- 15/21 [rich]\n",
      "   ---------------------------- ----------- 15/21 [rich]\n",
      "   ------------------------------ --------- 16/21 [typer]\n",
      "   ------------------------------ --------- 16/21 [typer]\n",
      "   ------------------------------ --------- 16/21 [typer]\n",
      "   ---------------------------------- ----- 18/21 [gradio-client]\n",
      "   ------------------------------------ --- 19/21 [fastapi]\n",
      "   ------------------------------------ --- 19/21 [fastapi]\n",
      "   ------------------------------------ --- 19/21 [fastapi]\n",
      "   ------------------------------------ --- 19/21 [fastapi]\n",
      "   ------------------------------------ --- 19/21 [fastapi]\n",
      "   -------------------------------------- - 20/21 [gradio]\n",
      "   -------------------------------------- - 20/21 [gradio]\n",
      "   -------------------------------------- - 20/21 [gradio]\n",
      "   -------------------------------------- - 20/21 [gradio]\n",
      "   -------------------------------------- - 20/21 [gradio]\n",
      "   -------------------------------------- - 20/21 [gradio]\n",
      "   -------------------------------------- - 20/21 [gradio]\n",
      "   -------------------------------------- - 20/21 [gradio]\n",
      "   -------------------------------------- - 20/21 [gradio]\n",
      "   -------------------------------------- - 20/21 [gradio]\n",
      "   -------------------------------------- - 20/21 [gradio]\n",
      "   -------------------------------------- - 20/21 [gradio]\n",
      "   -------------------------------------- - 20/21 [gradio]\n",
      "   -------------------------------------- - 20/21 [gradio]\n",
      "   -------------------------------------- - 20/21 [gradio]\n",
      "   -------------------------------------- - 20/21 [gradio]\n",
      "   -------------------------------------- - 20/21 [gradio]\n",
      "   -------------------------------------- - 20/21 [gradio]\n",
      "   -------------------------------------- - 20/21 [gradio]\n",
      "   -------------------------------------- - 20/21 [gradio]\n",
      "   -------------------------------------- - 20/21 [gradio]\n",
      "   -------------------------------------- - 20/21 [gradio]\n",
      "   -------------------------------------- - 20/21 [gradio]\n",
      "   -------------------------------------- - 20/21 [gradio]\n",
      "   -------------------------------------- - 20/21 [gradio]\n",
      "   -------------------------------------- - 20/21 [gradio]\n",
      "   -------------------------------------- - 20/21 [gradio]\n",
      "   -------------------------------------- - 20/21 [gradio]\n",
      "   -------------------------------------- - 20/21 [gradio]\n",
      "   -------------------------------------- - 20/21 [gradio]\n",
      "   -------------------------------------- - 20/21 [gradio]\n",
      "   -------------------------------------- - 20/21 [gradio]\n",
      "   -------------------------------------- - 20/21 [gradio]\n",
      "   -------------------------------------- - 20/21 [gradio]\n",
      "   -------------------------------------- - 20/21 [gradio]\n",
      "   ---------------------------------------- 21/21 [gradio]\n",
      "\n",
      "Successfully installed aiofiles-24.1.0 annotated-doc-0.0.4 brotli-1.2.0 fastapi-0.128.0 ffmpy-1.0.0 gradio-6.5.1 gradio-client-2.0.3 groovy-0.1.2 markdown-it-py-4.0.0 mdurl-0.1.2 orjson-3.11.6 pydub-0.25.1 python-multipart-0.0.22 rich-14.3.1 safehttpx-0.1.7 semantic-version-2.10.0 shellingham-1.5.4 starlette-0.50.0 tomlkit-0.13.3 typer-0.21.1 uvicorn-0.40.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.3 -> 26.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install gradio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38d9039",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7864\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7864/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR TRACEBACK:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\91979\\AppData\\Local\\Temp\\ipykernel_10860\\631478515.py\", line 6, in chat_interface\n",
      "    result = answer_query(user_query)\n",
      "NameError: name 'answer_query' is not defined. Did you mean: 'user_query'?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created dataset file at: .gradio\\flagged\\dataset1.csv\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import traceback\n",
    "\n",
    "def chat_interface(user_query):\n",
    "    try:\n",
    "        result = answer_query(user_query)\n",
    "        print(\"DEBUG result:\", result)  # shows in notebook output\n",
    "\n",
    "        # CASE 1: result is dict\n",
    "        if isinstance(result, dict):\n",
    "            return result.get(\"answer\", \"No 'answer' key found\")\n",
    "\n",
    "        # CASE 2: result is already string\n",
    "        if isinstance(result, str):\n",
    "            return result\n",
    "\n",
    "        return \"Unexpected return type\"\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"ERROR TRACEBACK:\")\n",
    "        traceback.print_exc()\n",
    "        return f\"Error occurred: {str(e)}\"\n",
    "\n",
    "gr.Interface(\n",
    "    fn=chat_interface,\n",
    "    inputs=gr.Textbox(label=\"Ask a question\"),\n",
    "    outputs=gr.Textbox(label=\"Answer\"),\n",
    "    title=\"Simple RAG Chatbot\"\n",
    ").launch(share=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf150104",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing midsemcode.py\n"
     ]
    }
   ],
   "source": [
    "# %%writefile midsemcode.py\n",
    "\n",
    "# # --------- COPY FROM HERE ---------\n",
    "\n",
    "# # Paste ALL imports that answer_query depends on\n",
    "# # Example (yours may have more):\n",
    "# import re\n",
    "# import numpy as np\n",
    "# from \n",
    "#  import SentenceTransformer\n",
    "# from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# # Paste any global variables/models used by answer_query\n",
    "# # Example:\n",
    "# # model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "# # sentences = [...]\n",
    "# # embeddings = [...]\n",
    "\n",
    "# def answer_query(query):\n",
    "#     # â¬‡â¬‡â¬‡â¬‡â¬‡â¬‡â¬‡â¬‡â¬‡â¬‡â¬‡â¬‡â¬‡â¬‡â¬‡â¬‡â¬‡â¬‡â¬‡â¬‡\n",
    "#     # COPY THE *ENTIRE CONTENT* OF YOUR\n",
    "#     # EXISTING answer_query FUNCTION HERE\n",
    "#     # EXACTLY AS IT IS\n",
    "#     # â¬†â¬†â¬†â¬†â¬†â¬†â¬†â¬†â¬†â¬†â¬†â¬†â¬†â¬†â¬†â¬†â¬†â¬†â¬†â¬†\n",
    "\n",
    "#     return result\n",
    "\n",
    "# # --------- END COPY ---------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b00fb8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_query(query):\n",
    "    # COPY EVERYTHING INSIDE YOUR EXISTING answer_query FUNCTION\n",
    "    # EXACTLY AS IT IS\n",
    "    return result\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
